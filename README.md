## llama-cpp-python-gradio-ui-server/localhost

**significant faster than cTransformers with the pre-loaded model approach
*comment out "share=True": localhost only

run it by (pull the pre-trained model file chat.gguf in the same directory):
```
python chat.py
```
