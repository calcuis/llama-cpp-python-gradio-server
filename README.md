## llama-cpp-python-gradio-ui-server

*comment out "share=True": localhost only

run it by (pull the pre-trained model file chat.gguf in the same directory):
```
python chat.py
```
